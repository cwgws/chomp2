{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get Collections\n",
    "\n",
    "__by Sean Gilleran__  \n",
    "__Last updated November 29__, __2021__  \n",
    "[https://github.com/seangilleran/ia-compmag-collect](https://github.com/seangilleran/ia-compmag-collect)\n",
    "\n",
    "This notebook gets a list of items from each collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Set Paths\n",
    "\n",
    "Set path to store metadata and tweak search URL if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"./meta\"\n",
    "collections_file = \"collections.txt\"\n",
    "\n",
    "search_url = \"https://archive.org/advancedsearch.php?q=collection:{collection}&fl[]=identifier&rows=999999&output=json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Set Collections\n",
    "\n",
    "List of collection identifiers to parse. You can load these from a text file or just type them in here as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List\n",
    "collections = []\n",
    "\n",
    "# File\n",
    "try:\n",
    "    with open(collections_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        collections.extend([l.strip() for l in f.readlines() if l.strip != \"\"])\n",
    "except Exception:\n",
    "    print(\"No collections file found.\")\n",
    "    pass\n",
    "\n",
    "print(\"\\n\".join(collections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Scrape & Store Collections\n",
    "\n",
    "Once everything is set up, run this cell to scrape items from collections. With that done, we can move on to collecting item metadata in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import requests\n",
    "\n",
    "collection_count = 0\n",
    "skip_count = 0\n",
    "item_count = 0\n",
    "\n",
    "\n",
    "# Create collections folder if it doesn't already exist.\n",
    "if not os.path.exists(metadata_path):\n",
    "    os.makedirs(os.path.abspath(metadata_path))\n",
    "\n",
    "\n",
    "for collection in collections:\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%X\")\n",
    "    collection_path = os.path.join(metadata_path, f\"{collection}.json\")\n",
    "\n",
    "    # Skip collection if we've already scraped it.\n",
    "    if os.path.exists(collection_path):\n",
    "        print(f\"[{timestamp}] Already scraped {collection}, skipping.\")\n",
    "        skip_count = skip_count + 1\n",
    "        continue\n",
    "\n",
    "    # Scrape a list of items from the collection.\n",
    "    print(f\"[{timestamp}] Getting items from {collection}...\")\n",
    "    collection_url = search_url.format(collection=collection)\n",
    "    items = []\n",
    "    #try:\n",
    "    r = requests.get(collection_url)\n",
    "    for item in r.json()[\"response\"][\"docs\"]:\n",
    "        items.append(item[\"identifier\"])\n",
    "        item_count = item_count + 1\n",
    "    #except Exception:\n",
    "    #    print(f\"[{timestamp}] Error parsing {collection}, skipping.\")\n",
    "     #   skip_count = skip_count + 1\n",
    "     #   continue\n",
    "\n",
    "    # Skip the collection if we can't find the item metadata.\n",
    "    if len(items) == 0:\n",
    "        print(f\"[{timestamp}] No items found in {collection}!\")\n",
    "        skip_count = skip_count + 1\n",
    "        continue\n",
    "\n",
    "    # Save the collection to JSON.\n",
    "    collection_meta = {\n",
    "        \"name\": collection,\n",
    "        \"id\": str(uuid.uuid5(uuid.NAMESPACE_DNS, collection)),\n",
    "        \"url\": collection_url,\n",
    "        \"items\": items\n",
    "    }\n",
    "    with open(collection_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(collection_meta, sort_keys=True, indent=4))\n",
    "    collection_count = collection_count + 1\n",
    "\n",
    "print(f\"\\n[{timestamp}] ** DONE **\")\n",
    "print(f\"Found {item_count} new items in {collection_count} collections ({skip_count} skipped).\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee8b9ecf46cc6e5ba1fee164dfe9a5d29badbac7e0088ca9fcb20f159d16cfe8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
