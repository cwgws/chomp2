{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Proccess Notebook\n","\n","__by Sean Gilleran__  \n","__Last updated November 28__, __2021__  \n","[https://github.com/seangilleran/ia-compmag-collect](https://github.com/seangilleran/ia-compmag-collect)\n","\n","This notebook processes raw text files into data more suitable to topic modelling. In order, the steps that it performs are:\n","\n","1. Load text files.\n","2. Replace Unicode characters with ASCII equivalents.\n","3. \"De-fuzz\" the text, i.e.:\n","   * Combine hyphenated words split across lines using Regex.\n","   * Remove isolated special characters (most of these are probably OCR artifacts).\n","   * Check and standardize spelling.\n","4. Tokenize the text into individual words.\n","5. Remove stopwords.\n","6. Replace lemmas with a common equivalent where possible.\n","7. Save the result to a new file."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Initialization"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Import & Initialization\n","\n","Python imports. Set up NLTK and other tools here. Make sure to run this before the other parts of the notebook!"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]}],"source":["from datetime import datetime\n","import os\n","\n","from autocorrect import Speller\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.tag import pos_tag\n","from nltk.tokenize import word_tokenize\n","import regex as re\n","from unidecode import unidecode\n","\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"universal_tagset\")\n","wnl = WordNetLemmatizer()\n","\n","dehyphenator = re.compile(r\"(?<=[A-Za-z])-\\s\\n(?=[A-Za-z])\")\n","defuzzer = re.compile(r\"([^a-zA-Z0-9]+)\")\n","\n","spell = Speller(only_replacements=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2 Input & Output Paths\n","\n","Files should be in `.txt` format in the `in_path` directory. Once processed, these will be duplicated into the `out_path` directory."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" in_path: g:\\sgill\\Development\\ia-compmag-collect\\corpus\\raw\n","out_path: g:\\sgill\\Development\\ia-compmag-collect\\corpus\\out\n"]}],"source":["in_path = \"./corpus/raw\"\n","out_path = \"./corpus/out\"\n","\n","print(f\" in_path: {os.path.abspath(in_path)}\")\n","print(f\"out_path: {os.path.abspath(out_path)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3 Add Stop Words\n","\n","Load the NLTK stop word list and append our own, if necessary."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'my', 'bei', 'piv', 'tht', 'mul', 'our', 'under', 'artd', 'itself', 'hou', 'where', 'rir', 'jun', 'yourselves', 'jiv', 'yoy', \"won't\", \"it's\", 'mil', 'doesn', 'than', \"shan't\", 'liie', 'weren', 'wilti', 'rhe', 'pvn', 'her', 'when', 'be', 'at', 'ihe', \"should've\", 'gama', 'jou', 'herself', 'fhe', 'haa', 'deg', \"you've\", 'hui', 'ihia', 'ma', 'haven', 'of', \"aren't\", 't', 'so', 'aren', 'bul', 'and', 'piu', 'tiave', 'ttii', 'just', 'itiey', 'iiri', 'not', 'm', 'ihat', 'from', 'few', 'their', 'has', 'up', 'only', 'hli', \"wasn't\", 'iib', \"wouldn't\", 'ttie', 'been', 'itie', 'lor', 'mustn', 'wtiich', 'needn', 'an', 'if', 'ttm', 'tha', 'he', 'doing', 'it', 'until', 'some', 'll', 've', 'hers', 'couldn', 'hadn', 'arki', 'iht', 'tiy', 'was', 'rtu', 'thi', 'isn', \"you'd\", \"needn't\", 'these', 'there', 'cai', 'noa', 'wos', 'ria', 'no', 'ond', 'ibe', 'giv', 'we', 'on', 'copr', 'ihey', 'here', 'any', 'rei', 'anfl', 'pue', 'all', 'vwi', \"hasn't\", 'after', 'for', 'does', 'what', 'mfi', 're', 'because', 'such', 'further', 'its', 'through', 'they', 'very', 'ihere', 'off', 'against', \"mightn't\", \"don't\", 'being', 'whom', 'svs', \"haven't\", 'or', 'a', 'him', 'bock', 'fen', 'before', 'mou', 'srt', 'how', 'aui', 'over', 'nor', 'are', 'wasn', 'whidi', 'once', 'iiil', 'you', 'do', 'own', 'your', 'ihfl', 'jin', 'thot', 'same', \"isn't\", 'his', 'dai', \"that'll\", 'vou', 'moke', 'who', 'did', 'as', \"couldn't\", 'aqj', 'wei', 'why', 'which', \"didn't\", \"hadn't\", 'shan', 'fig', 'were', 'have', 'with', 'while', \"you're\", 'other', 'don', 'gra', 'thm', 'above', 'o', 'during', 'having', 'youf', 'themselves', 'by', 'more', 'dnld', 'mightn', 'this', 'those', 'yourself', 'am', 'but', \"she's\", 'hia', 'vdu', 'tfw', 'ours', \"doesn't\", 'irv', 'jiu', \"weren't\", 'ihal', 'che', 'ihi', 'lof', 'gan', 'gome', 'khi', 'yours', 'in', 'can', 'itia', 'to', 'then', 'wouldn', 'eii', 'ifid', 'down', 'ilil', 'i', 'the', 'hasn', 'y', 'she', 'is', 'ind', 'most', 'won', 'each', 'below', 's', \"you'll\", 'jtf', 'between', 'again', 'aqi', 'ihf', 'them', 'that', 'both', 'me', 'fof', 'didn', 'ain', 'theirs', 'tfie', 'will', 'd', 'joj', 'about', 'out', 'iha', \"mustn't\", 'should', 'shouldn', 'myself', 'ourselves', 'himself', 'dei', \"shouldn't\", 'ttve', 'had', 'into', 'now', 'ehe', 'too', 'pub'}\n"]}],"source":["stop_words = set(stopwords.words(\"english\"))\n","\n","with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n","    stop_words.update([w.strip() for w in f.readlines()])\n","\n","print(stop_words)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Processing"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Find Files\n","\n","Look for raw files to process in the `out_path` directory. Check against the `in_path` directory to make sure we're not doubling up. This is also a handy way of being able to pause and resume our work."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 8841 files to process (443 skipped).\n"]}],"source":["files = []\n","total_count = 0\n","skip_count = 0\n","\n","for file in [f for f in os.listdir(in_path) if f.endswith(\".txt\")]:\n","\n","    if os.path.exists(os.path.join(out_path, file)):\n","        skip_count = skip_count + 1\n","        continue\n","\n","    files.append(file)\n","    total_count = total_count + 1\n","\n","print(f\"Found {total_count} files to process ({skip_count} skipped).\")"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Process Files\n","\n","This can take a very long time, especially with large data sets! We'll print out a message before each file with a note as to how far we've gotten."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Invalid format string","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16108/1647453639.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtimestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%-H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[{timestamp}] {i}/{total_count} ({((i / total_count)*100.0):.0f}%): {file}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: Invalid format string"]}],"source":["i = 0\n","for file in files:\n","\n","    timestamp = datetime.now().strftime(\"%X\")\n","    i = i + 1\n","    print(f\"[{timestamp}] {i}/{total_count} ({((i / total_count)*100.0):.0f}%): {file}\")\n","\n","    # Load file, remove special characters.\n","    text = \"\"\n","    with open(os.path.join(in_path, file), \"r\", encoding=\"utf-8\") as f:\n","        text = unidecode(f.read())\n","    if not text or text == None or text == \"\":\n","        print(\"No content in file!\")\n","        continue\n","\n","    # De-fuzz.\n","    text = dehyphenator.sub(\"\", text)\n","    text = defuzzer.sub(\" \", text)\n","    text = spell(text)\n","\n","    # Tokenize.\n","    tokenized_text = word_tokenize(text)\n","    text = []\n","\n","    # Remove stopwords.\n","    for word in [w for w in tokenized_text if w not in stop_words]:\n","        text.append(word)\n","\n","    # Lemmatize.\n","    text = pos_tag(text, tagset=\"universal\")\n","    for x in range(len(text)):\n","        word, pos = text[x]\n","        if pos == \"VERB\":\n","            pos = \"v\"\n","        elif pos == \"ADJ\":\n","            pos = \"a\"\n","        elif pos == \"ADV\":\n","            pos = \"r\"\n","        else:\n","            pos = \"n\"\n","        text[x] = wnl.lemmatize(word, pos=pos)\n","\n","    # Save updated text to new file.\n","    text = \" \".join(text)\n","    with open(os.path.join(out_path, file), \"w\") as f:\n","        f.write(text)\n","\n","print(\"** DONE **\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":2}
