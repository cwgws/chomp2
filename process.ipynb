{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Proccess Notebook\n","\n","__by Sean Gilleran__  \n","__Last updated November 28__, __2021__  \n","[https://github.com/seangilleran/ia-compmag-collect](https://github.com/seangilleran/ia-compmag-collect)\n","\n","This notebook processes raw text files into data more suitable to topic modelling. In order, the steps that it performs are:\n","\n","1. Load text files.\n","2. Replace Unicode characters with ASCII equivalents.\n","3. \"De-fuzz\" the text, i.e.:\n","   * Combine hyphenated words split across lines using Regex.\n","   * Remove isolated special characters (most of these are probably OCR artifacts).\n","   * Check and standardize spelling.\n","4. Tokenize the text into individual words.\n","5. Remove stopwords.\n","6. Replace lemmas with a common equivalent where possible.\n","7. Save the result to a new file."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Initialization"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Import & Initialization\n","\n","Python imports. Set up NLTK and other tools here. Make sure to run this before the other parts of the notebook!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datetime import datetime\n","import os\n","\n","from autocorrect import Speller\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.tag import pos_tag\n","from nltk.tokenize import word_tokenize\n","import regex as re\n","from unidecode import unidecode\n","\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"universal_tagset\")\n","wnl = WordNetLemmatizer()\n","\n","dehyphenator = re.compile(r\"(?<=[A-Za-z])-\\s\\n(?=[A-Za-z])\")\n","defuzzer = re.compile(r\"([^a-zA-Z0-9]+)\")\n","\n","spell = Speller(only_replacements=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2 Input & Output Paths\n","\n","Files should be in `.txt` format in the `in_path` directory. Once processed, these will be duplicated into the `out_path` directory."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["in_path = \"./corpus/raw\"\n","out_path = \"./corpus/out\"\n","\n","print(f\" in_path: {os.path.abspath(in_path)}\")\n","print(f\"out_path: {os.path.abspath(out_path)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3 Add Stop Words\n","\n","Load the NLTK stop word list and append our own, if necessary."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stop_words = set(stopwords.words(\"english\"))\n","\n","with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n","    stop_words.update([w.strip() for w in f.readlines()])\n","\n","print(stop_words)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Processing"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Find Files\n","\n","Look for raw files to process in the `out_path` directory. Check against the `in_path` directory to make sure we're not doubling up. This is also a handy way of being able to pause and resume our work."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["files = []\n","total_count = 0\n","skip_count = 0\n","\n","for file in [f for f in os.listdir(in_path) if f.endswith(\".txt\")]:\n","\n","    if os.path.exists(os.path.join(out_path, file)):\n","        skip_count = skip_count + 1\n","        continue\n","\n","    files.append(file)\n","    total_count = total_count + 1\n","\n","print(f\"Found {total_count} files to process ({skip_count} skipped).\")\n","\n","i = 1\n","t = len(files)\n","\n","print(f\"Found {t} files to process!\")"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Process Files\n","\n","This can take a very long time, especially with large data sets! We'll print out a message before each file with a note as to how far we've gotten."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for file in files:\n","\n","    print(f\"{i}/{t} ({(i / t):.0f}%): {file[:30]}...\")\n","    text = \"\"\n","\n","    # Load file, remove special characters.\n","    with open(os.path.join(in_path, file), \"r\", encoding=\"utf-8\") as f:\n","        text = unidecode(f.read())\n","    i = i + 1\n","\n","    # De-fuzz.\n","    text = dehyphenator.sub(\"\", text)\n","    text = defuzzer.sub(\" \", text)\n","    text = spell(text)\n","\n","    # Tokenize.\n","    tokenized_text = word_tokenize(text)\n","    text = []\n","\n","    # Remove stopwords.\n","    for word in [w for w in tokenized_text if w not in stop_words]:\n","        text.append(word)\n","\n","    # Lemmatize.\n","    text = pos_tag(text, tagset=\"universal\")\n","    for x in range(len(text)):\n","        word, pos = text[x]\n","        if pos == \"VERB\":\n","            pos = \"v\"\n","        elif pos == \"ADJ\":\n","            pos = \"a\"\n","        elif pos == \"ADV\":\n","            pos = \"r\"\n","        else:\n","            pos = \"n\"\n","        text[x] = wnl.lemmatize(word, pos=pos)\n","\n","    # Save updated text to new file.\n","    text = \" \".join(text)\n","    with open(os.path.join(out_path, file), \"w\") as f:\n","        f.write(text)\n","\n","print(\"** DONE **\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":2}
