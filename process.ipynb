{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Proccess Notebook\n","\n","__by Sean Gilleran__  \n","__Last updated November 28__, __2021__  \n","[https://github.com/seangilleran/ia-compmag-collect](https://github.com/seangilleran/ia-compmag-collect)\n","\n","This notebook processes raw text files into data more suitable to topic modelling. In order, the steps that it performs are:\n","\n","1. Load text files.\n","2. Replace Unicode characters with ASCII equivalents.\n","3. \"De-fuzz\" the text, i.e.:\n","   * Combine hyphenated words split across lines using Regex.\n","   * Remove isolated special characters (most of these are probably OCR artifacts).\n","   * Check and standardize spelling.\n","4. Tokenize the text into individual words.\n","5. Remove stopwords.\n","6. Replace lemmas with a common equivalent where possible.\n","7. Save the result to a new file."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Initialization"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Import & Initialization\n","\n","Python imports. Set up NLTK and other tools here. Make sure to run this before the other parts of the notebook!"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to\n","[nltk_data]     C:\\Users\\sgill\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]}],"source":["from datetime import datetime\n","import os\n","\n","from autocorrect import Speller\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.tag import pos_tag\n","from nltk.tokenize import word_tokenize\n","import regex as re\n","from unidecode import unidecode\n","\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"universal_tagset\")\n","wnl = WordNetLemmatizer()\n","\n","dehyphenator = re.compile(r\"(?<=[A-Za-z])-\\s\\n(?=[A-Za-z])\")\n","defuzzer = re.compile(r\"([^a-zA-Z0-9]+)\")\n","\n","spell = Speller(only_replacements=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2 Input & Output Paths\n","\n","Files should be in `.txt` format in the `IN_PATH` directory. Once processed, these will be duplicated into the `OUT_PATH` directory."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["IN_PATH = \"./corpus/raw\"\n","OUT_PATH = \"./corpus/out\""]},{"cell_type":"markdown","metadata":{},"source":["### 1.3 Add Stop Words\n","\n","Load the NLTK stop word list and append our own, if necessary."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'they', 'while', \"hasn't\", 'were', \"wouldn't\", 'a', 'vdu', 'd', 'ttii', 'mfi', 'tha', 'theirs', 'youf', \"won't\", 'themselves', 'won', \"mustn't\", 'srt', 'my', 'whom', \"aren't\", 'jou', 'ibe', 'll', 're', \"needn't\", 'aqi', 'iiri', 'and', 'joj', 'moke', \"wasn't\", \"you'd\", 'between', 'jin', 'then', 'itiey', 'pvn', 'ind', 'or', 'further', 'yourselves', 'than', 'che', 'from', 'ourselves', 'wouldn', 'ilil', 'where', 'iiil', 'wasn', 'hui', 'ifid', 'now', 'we', 'how', 'rei', 'bei', 'copr', 'your', 'very', \"should've\", 'few', 'haven', 'all', \"don't\", 'its', 'aren', 'against', 'ihal', 'no', 'ihe', 'again', 'deg', 'should', 'piu', 'most', 'piv', 'thm', 'their', 'not', 'is', 'hli', 'those', 'both', 'thi', 'o', 'about', 'y', 'pue', 'who', \"you're\", 'once', 'hasn', 'dnld', 'gama', 'some', 'rhe', 'his', 'itself', 'down', 'iha', 'me', \"hadn't\", 'wilti', 'needn', 'artd', 'yourself', \"doesn't\", 'why', 'can', 'tfw', 'jun', 'below', 'svs', 'mustn', 'ma', 'up', 'myself', 'nor', 'until', 'hou', 'mightn', 'does', 'himself', 'these', 'has', 'ihere', 'don', 'do', 'will', 'before', 'jiv', 'you', \"you've\", 'hers', 'haa', 'which', 'when', 'i', 'dai', 'wei', 'this', 'tiy', 'him', 'into', \"she's\", 'gome', 'be', \"weren't\", 'ain', 'an', \"you'll\", 'during', 'just', 'rir', 'here', 'itia', 'itie', 'isn', 'shouldn', 'what', 'own', 'mul', 'had', 'our', 'herself', 'so', 'are', \"it's\", 'of', 'for', 'such', 't', 'dei', 'eii', 'ihey', 'ttie', 'ttve', 'ihf', 'liie', 'but', 'arki', 'off', 'ond', 'couldn', 'been', 'he', 'on', 'tiave', 'am', 'aui', 'lof', 'under', 'ria', 'cai', 'vwi', 'doesn', 'tfie', 'doing', 'didn', 'khi', 'jtf', 'if', \"isn't\", 'bul', 'bock', 'being', 'too', 'at', 'out', 'tht', 'because', 've', 'her', 'ttm', 'wos', 'only', 'aqj', 'jiu', 'it', 'same', 'mil', 'weren', \"couldn't\", 'there', 'mou', 'she', 'fof', 'whidi', \"mightn't\", 'wtiich', 'by', 'gra', 'vou', 'fhe', 'ihi', \"didn't\", 'irv', 'thot', 'as', \"shouldn't\", 'each', 'did', 'yoy', 'noa', 'yours', 'after', 'to', 'over', 'that', 'pub', 'having', 'ehe', 'hia', 'in', 'have', 'other', \"haven't\", \"shan't\", 'iib', 'm', 'through', 'more', 's', 'lor', 'ihia', 'above', 'fen', 'iht', 'the', 'fig', 'ours', 'ihat', 'them', \"that'll\", 'anfl', 'with', 'shan', 'was', 'gan', 'ihfl', 'any', 'rtu', 'giv', 'hadn'}\n"]}],"source":["stop_words = set(stopwords.words(\"english\"))\n","\n","with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n","    stop_words.update([w.strip() for w in f.readlines()])\n","\n","print(stop_words)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Processing"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Find Files\n","\n","Look for raw files to process in the `OUT_PATH` directory. Check against the `IN_PATH` directory to make sure we're not doubling up. This is also a handy way of being able to pause and resume our work."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["files = []\n","total_count = 0\n","skip_count = 0\n","\n","for file in [f for f in os.listdir(IN_PATH) if f.endswith(\".txt\")]:\n","\n","    if os.exists(os.path.join(OUT_PATH, file)):\n","        skip_count = skip_count + 1\n","        continue\n","\n","    files.append(file)\n","    total_count = total_count + 1\n","\n","print(f\"Found {total_count} files to process ({skip_count} skipped).\")\n","\n","i = 1\n","t = len(files)\n","\n","print(f\"Found {t} files to process!\")"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Process Files\n","\n","This can take a very long time, especially with large data sets! We'll print out a message before each file with a note as to how far we've gotten."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["for file in files:\n","\n","    print(f\"{i}/{t} ({(i / t):.0f}%): {file[:30]}...\")\n","    text = \"\"\n","\n","    # Load file, remove special characters.\n","    with open(os.path.join(IN_PATH, file), \"r\", encoding=\"utf-8\") as f:\n","        text = unidecode(f.read())\n","    i = i + 1\n","\n","    # De-fuzz.\n","    text = dehyphenator.sub(\"\", text)\n","    text = defuzzer.sub(\" \", text)\n","    text = spell(text)\n","\n","    # Tokenize.\n","    tokenized_text = word_tokenize(text)\n","    text = []\n","\n","    # Remove stopwords.\n","    for word in [w for w in tokenized_text if w not in stop_words]:\n","        text.append(word)\n","\n","    # Lemmatize.\n","    text = pos_tag(text, tagset=\"universal\")\n","    for x in range(len(text)):\n","        word, pos = text[x]\n","        if pos == \"VERB\":\n","            pos = \"v\"\n","        elif pos == \"ADJ\":\n","            pos = \"a\"\n","        elif pos == \"ADV\":\n","            pos = \"r\"\n","        else:\n","            pos = \"n\"\n","        text[x] = wnl.lemmatize(word, pos=pos)\n","\n","    # Save updated text to new file.\n","    text = \" \".join(text)\n","    with open(os.path.join(OUT_PATH, file), \"w\") as f:\n","        f.write(text)\n","\n","print(\"** DONE **\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":2}
