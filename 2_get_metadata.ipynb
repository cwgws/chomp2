{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get Metadata\n",
    "\n",
    "__by Sean Gilleran__  \n",
    "__Last updated November 29__, __2021__  \n",
    "[https://github.com/seangilleran/ia-compmag-collect](https://github.com/seangilleran/ia-compmag-collect)\n",
    "\n",
    "This notebook gets metadata and a list of PDF and TXT files for each item in a collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def ts():\n",
    "    return f\"[{datetime.now().strftime('%X')}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Set Paths\n",
    "\n",
    "Set metadata path and tweak URLs if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"./meta\"\n",
    "\n",
    "meta_url = \"https://archive.org/metadata/{id}\"\n",
    "file_url = \"https://archive.org/download/{id}/{file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Scrape Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Find Items in Collections\n",
    "\n",
    "Find a list of collections to scrape. Ignore if we've already got metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load collections from JSON.\n",
    "# TODO: Catch JSON load errors.\n",
    "collections = []\n",
    "for file in [f for f in os.listdir(metadata_path) if f.endswith(\".json\")]:\n",
    "    with open(os.path.join(metadata_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        collections.append(json.load(f))\n",
    "\n",
    "# Assemble item URLs to scrape.\n",
    "scrapes = []\n",
    "item_count = 0\n",
    "items_skipped = 0\n",
    "collections_skipped = 0\n",
    "\n",
    "for collection in collections:\n",
    "\n",
    "    items = []\n",
    "    for item in collection[\"items\"]:\n",
    "        if not isinstance(item, str):\n",
    "            items_skipped = items_skipped + 1\n",
    "            continue\n",
    "        items.append(item)\n",
    "        item_count = item_count + 1\n",
    "\n",
    "    if len(items) == 0:\n",
    "        collections_skipped = 0\n",
    "        continue\n",
    "\n",
    "    collection[\"items\"] = items\n",
    "    scrapes.append(collection)\n",
    "\n",
    "print(\n",
    "    f\"Found {item_count} items ({items_skipped} skipped) \" \\\n",
    "    f\"from {len(scrapes)} collections ({collections_skipped} skipped).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Scrape Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_count = 0\n",
    "skip_count = 0\n",
    "file_count = 0\n",
    "file_skip_count = 0\n",
    "\n",
    "for collection in scrapes:\n",
    "    for item in collection[\"items\"]:\n",
    "\n",
    "        print(f\"{ts()} {collection['name']}/{item}\")\n",
    "        item_url = meta_url.format(id=item)\n",
    "        try:\n",
    "            r = requests.get(item_url)\n",
    "            data = r.json()\n",
    "        except Exception:\n",
    "            print(f\"{ts()} ERR: {item_url}\")\n",
    "            skip_count = skip_count + 1\n",
    "            continue\n",
    "\n",
    "        # If there aren't any files we can move on.\n",
    "        if \"files\" not in data.keys() or data[\"files\"] == \"\" or len(data[\"files\"]) == 0:\n",
    "            print(f\"{ts()} WRN: No items found in {collection['name']}!\")\n",
    "            skip_count = skip_count + 1\n",
    "            continue\n",
    "\n",
    "        # Store a list of files for scraping.\n",
    "        files = []\n",
    "        for file in [f for f in data[\"files\"] if f[\"name\"].endswith(\".txt\") or f[\"name\"].endswith(\".pdf\")]:\n",
    "            if file[\"format\"] == \"Metadata\":\n",
    "                file_skip_count = file_skip_count + 1\n",
    "                continue\n",
    "            files.append({\n",
    "                \"id\": str(uuid.uuid5(uuid.NAMESPACE_DNS, file[\"name\"])),\n",
    "                \"name\": file[\"name\"],\n",
    "                \"ext\": file[\"name\"][3:],\n",
    "                \"format\": file[\"format\"],\n",
    "                \"url\": file_url.format(id=item, file=file[\"name\"]),\n",
    "                \"size\": file[\"size\"],\n",
    "                \"md5\": file[\"md5\"],\n",
    "                \"crc32\": file[\"crc32\"],\n",
    "                \"sha1\": file[\"sha1\"],\n",
    "            })\n",
    "            file_count = file_count = 1\n",
    "\n",
    "        # Grab the rest of the item metadata.\n",
    "        metadata = {\n",
    "            \"id\": str(uuid.uuid5(uuid.NAMESPACE_DNS, item)),\n",
    "            \"name\": item,\n",
    "            \"title\": data[\"metadata\"].get(\"title\", \"\"),\n",
    "            \"date\": data[\"metadata\"].get(\"date\", \"\"),\n",
    "            \"language\": data[\"metadata\"].get(\"language\", \"\"),\n",
    "            \"tags\": data[\"metadata\"].get(\"subject\", []),\n",
    "            \"files\": files,\n",
    "        }\n",
    "\n",
    "        # Save this all back to the file.\n",
    "        with open(os.path.join(metadata_path, f\"{collection['name']}.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            collection_data = json.load(f)\n",
    "        for x in range(len(collection_data[\"items\"])):\n",
    "            if collection_data[\"items\"][x] == item:\n",
    "                collection_data[\"items\"][x] = metadata\n",
    "                break\n",
    "        with open(os.path.join(metadata_path, f\"{collection['name']}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(collection_data, sort_keys=True, indent=4))\n",
    "        item_count = item_count + 1\n",
    "\n",
    "print(f\"\\n{ts()} ** DONE! **\")\n",
    "print(\n",
    "    f\"Collected metadata for {item_count} items ({skip_count} skipped) \" \\\n",
    "    f\"and {file_count} files ({file_skip_count} skipped).\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee8b9ecf46cc6e5ba1fee164dfe9a5d29badbac7e0088ca9fcb20f159d16cfe8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
